
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>48. Supervised Deep Learning Example - Image Classification with the MNIST Dataset &#8212; DS 1300</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="49. Reinforcement Learning Example - Tic Tac Toe" href="18_workbook.html" />
    <link rel="prev" title="47. Unsupervised Learning Example - Folktale Clustering" href="16_workbook.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">DS 1300</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../book/00_introduction.html">
                    DS1300: A Practical Introduction to Data Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../book/01_data_science.html">
   1. Data Science Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../book/02_hpc.html">
   2. Introduction to High-Performance Computing
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../book/08_project.html">
   3. Semester Project
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../book/03_using_m2.html">
   4. Using ManeFrame II for Class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../book/04_github_and_initial_setup.html">
   5. Introduction to GitHub and Getting Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01_workbook.html">
   6. Introduction to Python Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../book/06_data_ethics_bias.html">
   12. Data Ethics and Bias
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_workbook.html">
   13. Working with Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01_assignment.html">
   14. Introduction to Python Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 4
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="04_workbook.html">
   20. Exploring and Cleaning a Data Set
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../book/10_pudding.html">
   23. The Pudding: Data Story Telling and Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_assignment.html">
   25. Assignment: Working with Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 5
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../book/09_modeling.html">
   26. Building Models with Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 6
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../book/11_dask_initial_setup.html">
   27. Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_workbook.html">
   28. Dask Delayed
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_workbook.html">
   29. Dask Arrays
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_workbook.html">
   30. Dask DataFrames
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 7
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../book/12_inference.html">
   31. Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_workbook.html">
   32. Data Storage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_workbook.html">
   33. Ordinary Least Squares
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_workbook.html">
   34. Prediction (out of sample)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_workbook.html">
   35. Quantile regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_assignment.html">
   36. Linear Regression and Temperature
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 8
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="12_workbook.html">
   38. Data Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15_workbook.html">
   39. GeoPandas and Mapping in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_workbook.html">
   40. Optimization Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_assignment.html">
   41. Data Visualization
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day 9
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../book/13_art.html">
   42. Art
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14_workbook.html">
   43. Intermediate TextMining with Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../book/14_intro_to_AI.html">
   46. Introduction to AI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="16_workbook.html">
   47. Unsupervised Learning Example - Folktale Clustering
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   48. Supervised Deep Learning Example - Image Classification with the MNIST Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="18_workbook.html">
   49. Reinforcement Learning Example - Tic Tac Toe
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/code/17_workbook.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-mnist-dataset">
   48.1. The MNIST Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-and-validation-data-and-labels">
   48.2. Training and Validation Data and Labels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-the-data-into-memory-with-keras">
   48.3. Loading the Data Into Memory (with Keras)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploring-the-mnist-data">
   48.4. Exploring the MNIST Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-data-for-training">
   48.5. Preparing the Data for Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#flattening-the-image-data">
     48.5.1. Flattening the Image Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalizing-the-image-data">
     48.5.2. Normalizing the Image Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-encoding">
     48.5.3. Categorical Encoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorically-encoding-the-labels">
     48.5.4. Categorically Encoding the Labels
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-the-model">
   48.6. Creating the Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instantiating-the-model">
     48.6.1. Instantiating the Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-input-layer">
     48.6.2. Creating the Input Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-hidden-layer">
     48.6.3. Creating the Hidden Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-output-layer">
     48.6.4. Creating the Output Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summarizing-the-model">
     48.6.5. Summarizing the Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compiling-the-model">
     48.6.6. Compiling the Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-the-model">
   48.7. Training the Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observing-accuracy">
     48.7.1. Observing Accuracy
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Supervised Deep Learning Example - Image Classification with the MNIST Dataset</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-mnist-dataset">
   48.1. The MNIST Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-and-validation-data-and-labels">
   48.2. Training and Validation Data and Labels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-the-data-into-memory-with-keras">
   48.3. Loading the Data Into Memory (with Keras)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploring-the-mnist-data">
   48.4. Exploring the MNIST Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-data-for-training">
   48.5. Preparing the Data for Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#flattening-the-image-data">
     48.5.1. Flattening the Image Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normalizing-the-image-data">
     48.5.2. Normalizing the Image Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorical-encoding">
     48.5.3. Categorical Encoding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorically-encoding-the-labels">
     48.5.4. Categorically Encoding the Labels
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-the-model">
   48.6. Creating the Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#instantiating-the-model">
     48.6.1. Instantiating the Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-input-layer">
     48.6.2. Creating the Input Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-hidden-layer">
     48.6.3. Creating the Hidden Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-output-layer">
     48.6.4. Creating the Output Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summarizing-the-model">
     48.6.5. Summarizing the Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compiling-the-model">
     48.6.6. Compiling the Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-the-model">
   48.7. Training the Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observing-accuracy">
     48.7.1. Observing Accuracy
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <!-- #region tags=[] -->
<section class="tex2jax_ignore mathjax_ignore" id="supervised-deep-learning-example-image-classification-with-the-mnist-dataset">
<h1><span class="section-number">48. </span>Supervised Deep Learning Example - Image Classification with the MNIST Dataset<a class="headerlink" href="#supervised-deep-learning-example-image-classification-with-the-mnist-dataset" title="Permalink to this headline">#</a></h1>
<!-- #endregion -->
<section id="the-mnist-dataset">
<h2><span class="section-number">48.1. </span>The MNIST Dataset<a class="headerlink" href="#the-mnist-dataset" title="Permalink to this headline">#</a></h2>
<p>In the history of deep learning, the accurate image classification of the <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>, a collection of 70,000 grayscale images of handwritten digits from 0 to 9, was a major development. While today the problem is considered trivial, doing image classification with MNIST has become a kind of “Hello World” for deep learning.</p>
<p>Here are 40 of the images included in the MNIST dataset:</p>
<img src="../book/images/mnist1.png" style="width: 600px;">
</section>
<section id="training-and-validation-data-and-labels">
<h2><span class="section-number">48.2. </span>Training and Validation Data and Labels<a class="headerlink" href="#training-and-validation-data-and-labels" title="Permalink to this headline">#</a></h2>
<p>When working with images for deep learning, we need both the images themselves, usually denoted as <code class="docutils literal notranslate"><span class="pre">X</span></code>, and also, correct <a class="reference external" href="https://developers.google.com/machine-learning/glossary#label">labels</a> for these images, usually denoted as <code class="docutils literal notranslate"><span class="pre">Y</span></code>. Furthermore, we need <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code> values both for <em>training</em> the model, and then, a separate set of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code> values for <em>validating</em> the performance of the model after it has been trained. Therefore, we need 4 segments of data for the MNIST dataset:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x_train</span></code>: Images used for training the neural network</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_train</span></code>: Correct labels for the <code class="docutils literal notranslate"><span class="pre">x_train</span></code> images, used to evaluate the model’s predictions during training</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x_valid</span></code>: Images set aside for validating the performance of the model after it has been trained</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">y_valid</span></code>: Correct labels for the <code class="docutils literal notranslate"><span class="pre">x_valid</span></code> images, used to evaluate the model’s predictions after it has been trained</p></li>
</ol>
<p>The process of preparing data for analysis is called <a class="reference external" href="https://medium.com/&#64;rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7">Data Engineering</a>. To learn more about the differences between training data and validation data (as well as test data), check out <a class="reference external" href="https://machinelearningmastery.com/difference-test-validation-datasets/">this article</a> by Jason Brownlee.</p>
</section>
<section id="loading-the-data-into-memory-with-keras">
<h2><span class="section-number">48.3. </span>Loading the Data Into Memory (with Keras)<a class="headerlink" href="#loading-the-data-into-memory-with-keras" title="Permalink to this headline">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
</pre></div>
</div>
<p>With the <code class="docutils literal notranslate"><span class="pre">mnist</span></code> module, we can easily load the MNIST data, already partitioned into images and labels for both training and validation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the data, split between train and validation sets</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="exploring-the-mnist-data">
<h2><span class="section-number">48.4. </span>Exploring the MNIST Data<a class="headerlink" href="#exploring-the-mnist-data" title="Permalink to this headline">#</a></h2>
<p>We stated above that the MNIST dataset contained 70,000 grayscale images of handwritten digits. By executing the following cells, we can see that Keras has partitioned 60,000 of these images for training, and 10,000 for validation (after training), and also, that each image itself is a 2D array with the dimensions 28x28:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_valid</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<p>Furthermore, we can see that these 28x28 images are represented as a collection of unsigned 8-bit integer values between 0 and 255, the values corresponding with a pixel’s grayscale value where <code class="docutils literal notranslate"><span class="pre">0</span></code> is black, <code class="docutils literal notranslate"><span class="pre">255</span></code> is white, and all other values are in between:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Using <a class="reference external" href="https://matplotlib.org/">Matplotlib</a>, we can render one of these grayscale images in our dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In this way we can now see that this is a 28x28 pixel image of a 5. Or is it a 3? The answer is in the <code class="docutils literal notranslate"><span class="pre">y_train</span></code> data, which contains correct labels for the data. Let’s take a look:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="preparing-the-data-for-training">
<h2><span class="section-number">48.5. </span>Preparing the Data for Training<a class="headerlink" href="#preparing-the-data-for-training" title="Permalink to this headline">#</a></h2>
<p>In deep learning, it is common that data needs to be transformed to be in the ideal state for training. For this particular image classification problem, there are 3 tasks we should perform with the data in preparation for training:</p>
<ol class="simple">
<li><p>Flatten the image data, to simplify the image input into the model</p></li>
<li><p>Normalize the image data, to make the image input values easier to work with for the model</p></li>
<li><p>Categorize the labels, to make the label values easier to work with for the model</p></li>
</ol>
<section id="flattening-the-image-data">
<h3><span class="section-number">48.5.1. </span>Flattening the Image Data<a class="headerlink" href="#flattening-the-image-data" title="Permalink to this headline">#</a></h3>
<p>Though it’s possible for a deep learning model to accept a 2-dimensional image (in our case 28x28 pixels), we’re going to simplify things to start and <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/reshape">reshape</a> each image into a single array of 784 continuous pixels (note: 28x28 = 784). This is also called flattening the image.</p>
<p>Here we accomplish this using the helper method <code class="docutils literal notranslate"><span class="pre">reshape</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">x_valid</span> <span class="o">=</span> <span class="n">x_valid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
</pre></div>
</div>
<p>We can confirm that the image data has been reshaped and is now a collection of 1D arrays containing 784 pixel values each:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="normalizing-the-image-data">
<h3><span class="section-number">48.5.2. </span>Normalizing the Image Data<a class="headerlink" href="#normalizing-the-image-data" title="Permalink to this headline">#</a></h3>
<p>Deep learning models are better at dealing with floating point numbers between 0 and 1 (more on this topic later). Converting integer values to floating point values between 0 and 1 is called <a class="reference external" href="https://developers.google.com/machine-learning/glossary#normalization">normalization</a>, and a simple approach we will take here to normalize the data will be to divide all the pixel values (which if you recall are between 0 and 255) by 255:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">x_valid</span> <span class="o">=</span> <span class="n">x_valid</span> <span class="o">/</span> <span class="mi">255</span> 
</pre></div>
</div>
<p>We can now see that the values are all floating point values between <code class="docutils literal notranslate"><span class="pre">0.0</span></code> and <code class="docutils literal notranslate"><span class="pre">1.0</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="categorical-encoding">
<h3><span class="section-number">48.5.3. </span>Categorical Encoding<a class="headerlink" href="#categorical-encoding" title="Permalink to this headline">#</a></h3>
<p>Consider for a moment, if we were to ask, what is 7 - 2? Stating that the answer was 4 is closer than stating that the answer was 9. However, for this image classification problem, we don’t want the neural network to learn this kind of reasoning: we just want it to select the correct category, and understand that if we have an image of the number 5, that guessing 4 is just as bad as guessing 9.</p>
<p>As it stands, the labels for the images are integers between 0 and 9. Because these values represent a numerical range, the model might try to draw some conclusions about its performance based on how close to the correct numerical category it guesses.</p>
<p>Therefore, we will do something to our data called categorical encoding. This kind of transformation modifies the data so that each value is a collection of all possible categories, with the actual category that this particular value is set as true.</p>
<p>As a simple example, consider if we had 3 categories: red, blue, and green. For a given color, 2 of these categories would be false, and the other would be true:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Actual Color</p></th>
<th class="head"><p>Is Red?</p></th>
<th class="head"><p>Is Blue?</p></th>
<th class="head"><p>Is Green?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Red</p></td>
<td><p>True</p></td>
<td><p>False</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>Green</p></td>
<td><p>False</p></td>
<td><p>False</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>Blue</p></td>
<td><p>False</p></td>
<td><p>True</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>Green</p></td>
<td><p>False</p></td>
<td><p>False</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p>Rather than use “True” or “False”, we could represent the same using binary, either 0 or 1:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Actual Color</p></th>
<th class="head"><p>Is Red?</p></th>
<th class="head"><p>Is Blue?</p></th>
<th class="head"><p>Is Green?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Red</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Green</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>Blue</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>Green</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>This is what categorical encoding is, transforming values which are intended to be understood as categorical labels into a representation that makes their categorical nature explicit to the model. Thus, if we were using these values for training, we would convert…</p>
<!-- #region -->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;red, green, blue, green&#39;</span><span class="p">]</span>
</pre></div>
</div>
<!-- #endregion -->
<p>… which a neural network would have a very difficult time making sense of, instead to:</p>
<!-- #region -->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">]</span>
</pre></div>
</div>
<!-- #endregion -->
</section>
<section id="categorically-encoding-the-labels">
<h3><span class="section-number">48.5.4. </span>Categorically Encoding the Labels<a class="headerlink" href="#categorically-encoding-the-labels" title="Permalink to this headline">#</a></h3>
<p>Keras provides a utility to <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical">categorically encode values</a>, and here we use it to perform categorical encoding for both the training and validation labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="nn">keras</span>
<span class="n">num_categories</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_categories</span><span class="p">)</span>
<span class="n">y_valid</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">num_categories</span><span class="p">)</span>
</pre></div>
</div>
<p>Here are the first 10 values of the training labels, which you can see have now been categorically encoded:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">9</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="creating-the-model">
<h2><span class="section-number">48.6. </span>Creating the Model<a class="headerlink" href="#creating-the-model" title="Permalink to this headline">#</a></h2>
<p>With the data prepared for training, it is now time to create the model that we will train with the data. This first basic model will be made up of several <em>layers</em> and will be comprised of 3 main parts:</p>
<ol class="simple">
<li><p>An input layer, which will receive data in some expected format</p></li>
<li><p>Several <a class="reference external" href="https://developers.google.com/machine-learning/glossary#hidden-layer">hidden layers</a>, each comprised of many <em>neurons</em>. Each <a class="reference external" href="https://developers.google.com/machine-learning/glossary#neuron">neuron</a> will have the ability to affect the network’s guess with its <em>weights</em>, which are values that will be updated over many iterations as the network gets feedback on its performance and learns</p></li>
<li><p>An output layer, which will depict the network’s guess for a given image</p></li>
</ol>
<section id="instantiating-the-model">
<h3><span class="section-number">48.6.1. </span>Instantiating the Model<a class="headerlink" href="#instantiating-the-model" title="Permalink to this headline">#</a></h3>
<p>To begin, we will use Keras’s <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential">Sequential</a> model class to instantiate an instance of a model that will have a series of layers that data will pass through in sequence:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="creating-the-input-layer">
<h3><span class="section-number">48.6.2. </span>Creating the Input Layer<a class="headerlink" href="#creating-the-input-layer" title="Permalink to this headline">#</a></h3>
<p>Next, we will add the input layer. This layer will be <em>densely connected</em>, meaning that each neuron in it, and its weights, will affect every neuron in the next layer. To do this with Keras, we use Keras’s <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">Dense</a> layer class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">units</span></code> argument specifies the number of neurons in the layer. We are going to use <code class="docutils literal notranslate"><span class="pre">512</span></code> which we have chosen from experimentation. Choosing the correct number of neurons is what puts the “science” in “data science” as it is a matter of capturing the statistical complexity of the dataset. Try playing around with this value later to see how it affects training and to start developing a sense for what this number means.</p>
<p>We will learn more about activation functions later, but for now, we will use the <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation function, which in short, will help our network to learn how to make more sophisticated guesses about data than if it were required to make guesses based on some strictly linear function.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> value specifies the shape of the incoming data which in our situation is a 1D array of 784 values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>
</pre></div>
</div>
</section>
<section id="creating-the-hidden-layer">
<h3><span class="section-number">48.6.3. </span>Creating the Hidden Layer<a class="headerlink" href="#creating-the-hidden-layer" title="Permalink to this headline">#</a></h3>
<p>Now we will add an additional densely connected layer. Again, much more will be said about these later, but for now know that these layers give the network more parameters to contribute towards its guesses, and therefore, more subtle opportunities for accurate learning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="creating-the-output-layer">
<h3><span class="section-number">48.6.4. </span>Creating the Output Layer<a class="headerlink" href="#creating-the-output-layer" title="Permalink to this headline">#</a></h3>
<p>Finally, we will add an output layer. This layer uses the activation function <code class="docutils literal notranslate"><span class="pre">softmax</span></code> which will result in each of the layer’s values being a probability between 0 and 1 and will result in all the outputs of the layer adding to 1. In this case, since the network is to make a guess about a single image belonging to 1 of 10 possible categories, there will be 10 outputs. Each output gives the model’s guess (a probability) that the image belongs to that specific class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="summarizing-the-model">
<h3><span class="section-number">48.6.5. </span>Summarizing the Model<a class="headerlink" href="#summarizing-the-model" title="Permalink to this headline">#</a></h3>
<p>Keras provides the model instance method <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/summary">summary</a> which will print a readable summary of a model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p>Note the number of trainable parameters. Each of these can be adjusted during training and will contribute towards the trained model’s guesses.</p>
</section>
<section id="compiling-the-model">
<h3><span class="section-number">48.6.6. </span>Compiling the Model<a class="headerlink" href="#compiling-the-model" title="Permalink to this headline">#</a></h3>
<p>Again, more details are to follow, but the final step we need to do before we can actually train our model with data is to <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile">compile</a> it. Here we specify a <a class="reference external" href="https://developers.google.com/machine-learning/glossary#loss">loss function</a> which will be used for the model to understand how well it is performing during training. We also specify that we would like to track <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> while the model trains:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="training-the-model">
<h2><span class="section-number">48.7. </span>Training the Model<a class="headerlink" href="#training-the-model" title="Permalink to this headline">#</a></h2>
<p>Now that we have prepared training and validation data, and a model, it’s time to train our model with our training data, and verify it with its validation data.</p>
<p>“Training a model with data” is often also called “fitting a model to data.” Put this latter way, it highlights that the shape of the model changes over time to more accurately understand the data that it is being given.</p>
<p>When fitting (training) a model with Keras, we use the model’s <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit">fit</a> method. It expects the following arguments:</p>
<ul class="simple">
<li><p>The training data</p></li>
<li><p>The labels for the training data</p></li>
<li><p>The number of times it should train on the entire training dataset (called an <em>epoch</em>)</p></li>
<li><p>The validation or test data, and its labels</p></li>
</ul>
<p>Run the cell below to train the model. We will discuss its output after the training completes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<section id="observing-accuracy">
<h3><span class="section-number">48.7.1. </span>Observing Accuracy<a class="headerlink" href="#observing-accuracy" title="Permalink to this headline">#</a></h3>
<p>For each of the 5 epochs, notice the <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> and <code class="docutils literal notranslate"><span class="pre">val_accuracy</span></code> scores. <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> states how well the model did for the epoch on all the training data. <code class="docutils literal notranslate"><span class="pre">val_accuracy</span></code> states how well the model did on the validation data, which if you recall, was not used at all for training the model.</p>
<p>The model did quite well! The accuracy quickly reached close to 100%, as did the validation accuracy. We now have a model that can be used to accurately detect and classify hand-written images.</p>
<p>The next step would be to use this model to classify new not-yet-seen handwritten images. This is called <a class="reference external" href="https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/">inference</a>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./code"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="16_workbook.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">47. </span>Unsupervised Learning Example - Folktale Clustering</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="18_workbook.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">49. </span>Reinforcement Learning Example - Tic Tac Toe</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>