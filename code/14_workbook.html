

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Intermediate TextMining with Python &#8212; DS 1300</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'code/14_workbook';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Visualization" href="04_assignment.html" />
    <link rel="prev" title="Data Visualization" href="12_workbook.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../book/00_introduction.html">
  
  
  
  
  
    <p class="title logo__title">DS 1300</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../book/00_introduction.html">
                    DS1300: A Practical Introduction to Data Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/01_hpc.html">Introduction to High-Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/02_using_hpc.html">Using M3 for Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/03_github_and_initial_setup.html">Introduction to GitHub and Getting Setup</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/04_data_science.html">Data Science Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/08_project.html">Semester Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_workbook.html">Introduction to Python Programming</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/10_pudding.html">The Pudding: Data Story Telling and Visualization</a></li>

<li class="toctree-l1"><a class="reference internal" href="02_workbook.html">Working with Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_assignment.html">Introduction to Python Programming</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/06_data_ethics_bias.html">Data Ethics and Bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_workbook.html">Exploring and Cleaning a Data Set</a></li>


<li class="toctree-l1"><a class="reference internal" href="02_assignment.html">Assignment: Working with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/09_modeling.html">Building Models with Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="05_workbook.html">Dask Delayed</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_workbook.html">Dask Arrays</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_workbook.html">Dask DataFrames</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/12_inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_workbook.html">GeoPandas and Mapping in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_assignment.html">Linear Regression and Temperature</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 8</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="12_workbook.html">Data Visualization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Intermediate TextMining with Python</a></li>


<li class="toctree-l1"><a class="reference internal" href="04_assignment.html">Data Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="13_workbook.html">Optimization Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../book/14_intro_to_AI.html">Introduction to AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_workbook.html">Unsupervised Learning Example - Folktale Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_workbook.html">Supervised Deep Learning Example - Image Classification with the MNIST Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_workbook.html">Reinforcement Learning Example - Tic Tac Toe</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../book/13_art.html">Art</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/code/14_workbook.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Intermediate TextMining with Python</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Intermediate TextMining with Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-words-and-characters">Counting Words and Characters</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#processing-text">Processing Text</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cleaning-up-words">Cleaning Up Words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lowercase">Lowercase</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-punctuation">Remove Punctuation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-stopwords">Remove Stopwords</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-frequent-words">Remove Frequent Words</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lemmatization">Lemmatization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-text-processing">Advanced Text Processing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams">N-grams</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency">Term Frequency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-document-frequency">Inverse Document Frequency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-inverse-document-frequency-tf-idf">Term Frequency – Inverse Document Frequency (TF-IDF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity">Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jaccard-similarity">Jaccard Similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine Similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-similarity">Visualize Similarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">Bag of Words</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis">Sentiment Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-tf-idf-and-machine-learning">Using TF-IDF and Machine Learning</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="intermediate-textmining-with-python">
<h1>Intermediate TextMining with Python<a class="headerlink" href="#intermediate-textmining-with-python" title="Permalink to this heading">#</a></h1>
<p>By: Dr. Eric Godat and Dr. Rob Kalescky</p>
<p>Adapted from: <a class="reference external" href="https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/">Ultimate Guide to deal with Text Data (Using Python)</a></p>
<p>Natural Language Toolkit: <a class="reference external" href="http://www.nltk.org/">Documentation</a></p>
<p>Reference Text: <a class="reference external" href="http://www.nltk.org/book/">Natural Language Processing with Python</a></p>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">#</a></h2>
<p>These are the basic libraries we will use in for data manipulation (pandas) and math functions (numpy). We will add more libraries as we need them.</p>
<p>As a best practice, it is a good idea to load all your libraries in a single cell at the top of a notebook, however for the purposes of this tutorial we will load some now and more as we go.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span><span class="s1">&#39;retina&#39;</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">glob</span>
</pre></div>
</div>
<p>Load a data file into a pandas DataFrame.</p>
<p>This tutorial was designed around using sets of data you have yourselves in a form like a CSV, TSV, or TXT file.  Feel free to use any set of data, but for now we will use a dataset created from scraping this <a class="reference external" href="http://www.mftd.org/">Multilingual Folktale Database</a>.</p>
<p>This file is a CSV filetype, common for text data, but your data may also be stored as TSV’s, TXT’s, or other file types.  This will slightly change how you read from Pandas, but the concept is largely the same for the different filetypes.  Just keep this in mind when you see references to CSV.</p>
<p>To proceed, you will need to have this file downloaded and in the same folder as this notebook. Alternatively you can put the full path to the file.  Typically, your program will look for the file with the name you specified in the folder that contains your program unless you give the program a path to follow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;../data/folktales.csv&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">:</span><span class="s1">&#39;Index&#39;</span><span class="p">},</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#filename = &#39;/scratch/group/oit_research_data/wikiplots/wikiplots.csv&#39;#If you need to put the path to the file, do so here.</span>
<span class="c1">#data = pd.read_csv(filename)</span>
<span class="c1">#data.rename(columns={&#39;Unnamed: 0&#39;:&#39;Index&#39;},inplace=True)</span>
<span class="c1">#data.rename(columns={&#39;Plot&#39;:&#39;Story&#39;},inplace=True)</span>
<span class="c1">#data = data.sample(2000)</span>
<span class="c1">#data.head()</span>
</pre></div>
</div>
<p>Here we can see all the information available to us from the file in the form of a Pandas DataFrame. For the remainder of this tutorial, we will focus primarily on the full text of each data chunk, which we will name the <em>Story</em> column.  With your data set this is likely to be something very different, so feel free to call is something else.</p>
</section>
<section id="counting-words-and-characters">
<h2>Counting Words and Characters<a class="headerlink" href="#counting-words-and-characters" title="Permalink to this heading">#</a></h2>
<p>The first bit of analysis we might want to do is to count the number of words in one piece of data. To do this we will add a column called <em>wordcount</em> and write an operation that applies a function to every row of the column.</p>
<p>Unpacking this piece of code, <em>len(str(x).split(” “)</em>, tells us what is happening.</p>
<p>For the content of cell <em>x</em>, convert it to a string, <em>str()</em>, then split that string into pieces at each space, <em>split()</em>.</p>
<p>The result of that is a list of all the words in the text and then we can count the length of that list, <em>len()</em>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;wordcount&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)))</span>
<span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Story&#39;</span><span class="p">,</span><span class="s1">&#39;wordcount&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>We can do something similar to count the number of characters in the data chunk, including spaces. If you wanted to exclude whitespaces, you could take the list we made above, join it together and count the length of the resulting string.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s2">&quot;No Information Provided&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;char_count&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span>
<span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Story&#39;</span><span class="p">,</span><span class="s1">&#39;char_count&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>Now we want to calculate the average word length in the data.</p>
<p>Let’s define a function that will do that for us:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">avg_word</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
</pre></div>
</div>
<p>We can now apply that function to all the data chunks and save that in a new column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;avg_word&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">avg_word</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Story&#39;</span><span class="p">,</span><span class="s1">&#39;avg_word&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>We can then sort by the average word length.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Story&#39;</span><span class="p">,</span><span class="s1">&#39;avg_word&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;avg_word&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="processing-text">
<h1>Processing Text<a class="headerlink" href="#processing-text" title="Permalink to this heading">#</a></h1>
<p>A major component of doing analysis on text is the cleaning of the text prior to the analysis.</p>
<p>Though this process destroys some elements of the text (sentence structure, for example), it is often necessary in order to describe a text analytically. Depending on your choice of cleaning techniques, some elements might be preserved better than others if that is of importance to your analysis.</p>
<section id="cleaning-up-words">
<h2>Cleaning Up Words<a class="headerlink" href="#cleaning-up-words" title="Permalink to this heading">#</a></h2>
<p>This series of steps aims to clean up and standardize the text itself. This generally consists of removing common elements such as stopwords and punctuation but can be expanded to more detailed removals.</p>
<section id="lowercase">
<h3>Lowercase<a class="headerlink" href="#lowercase" title="Permalink to this heading">#</a></h3>
<p>Here we enforce that all of the text is lowercase. This makes it easier to match cases and sort words.</p>
<p>Notice we are assigning our modified column back to itself. This will save our modifications to our DataFrame</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="remove-punctuation">
<h3>Remove Punctuation<a class="headerlink" href="#remove-punctuation" title="Permalink to this heading">#</a></h3>
<p>Here we remove all punctuation from the data. This allows us to focus on the words only as well as assist in matching.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="remove-stopwords">
<h3>Remove Stopwords<a class="headerlink" href="#remove-stopwords" title="Permalink to this heading">#</a></h3>
<p>Stopwords are words that are commonly used and do little to aid in the understanding of the content of a text. There is no universal list of stopwords and they vary on the style, time period and media from which your text came from.  Typically, people choose to remove stopwords from their data, as it adds extra clutter while the words themselves provide little to no insight as to the nature of the data.  For now, we are simply going to count them to get an idea of how many there are.</p>
<p>For this tutorial, we will use the standard list of stopwords provided by the Natural Language Toolkit python library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#from nltk.corpus import stopwords</span>
<span class="c1">#stop = stopwords.words(&#39;english&#39;)</span>
<span class="c1">#data[&#39;Story&#39;] = data[&#39;Story&#39;].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in stop))</span>
<span class="c1">#data[&#39;Story&#39;].head()</span>
</pre></div>
</div>
</section>
<section id="remove-frequent-words">
<h3>Remove Frequent Words<a class="headerlink" href="#remove-frequent-words" title="Permalink to this heading">#</a></h3>
<p>If we want to catch common words that might have slipped through the stopword removal, we can build out a list of the most common words remaining in our text.</p>
<p>Here we have built a list of the 10 most common words. Some of these words might actually be relevant to our analysis so it is important to be careful with this method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#freq = pd.Series(&#39; &#39;.join(data[&#39;Story&#39;]).split()).value_counts()[:10]</span>
<span class="c1">#freq</span>
</pre></div>
</div>
<p>We now follow the same procedure with which we removed stopwords to remove the most frequent words.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#freq = list(freq.index)</span>
<span class="c1">#data[&#39;Story&#39;] = data[&#39;Story&#39;].apply(lambda x: &quot; &quot;.join(x for x in x.split() if x not in freq))</span>
<span class="c1">#data[&#39;Story&#39;].head()</span>
</pre></div>
</div>
</section>
</section>
<section id="lemmatization">
<h2>Lemmatization<a class="headerlink" href="#lemmatization" title="Permalink to this heading">#</a></h2>
<p>Lemmatization is often a more useful approach than stemming because it leverages an understanding of the word itself to convert the word back to its root word. However, this means lemmatization is less aggressive than stemming (probably a good thing).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#import nltk</span>
<span class="c1">#nltk.download(&#39;wordnet&#39;)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">textblob</span> <span class="kn">import</span> <span class="n">Word</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">Word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()]))</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>At this point we have a several options for cleaning and structuring our text data. The next section will focus on more advanced ways to study text analytically.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="advanced-text-processing">
<h1>Advanced Text Processing<a class="headerlink" href="#advanced-text-processing" title="Permalink to this heading">#</a></h1>
<p>This section focuses on more complex methods of analyzing textual data. We will continue to work with our same DataFrame.</p>
<section id="n-grams">
<h2>N-grams<a class="headerlink" href="#n-grams" title="Permalink to this heading">#</a></h2>
<p>N-grams are combinations of multiple words as they appear in the text. The N refers to the number of words captured in the list. N-grams with N=1 are referred unigrams and are just a nested list of all the words in the text. Following a similar pattern, bigrams (N=2), trigrams (N=3), etc. can be used.</p>
<p>N-grams allow you to capture the structure of the text which can be very useful. For instance, counting the number of bigrams where “said” was preceded by “he” vs “she” could give you an idea of the gender breakdown of speakers in a text. However, if you make your N-grams too long, you lose the ability to make comparisons.</p>
<p>Another concern, especially in very large data sets, is that the memory storage of N-grams scales with N (bigrams are twice as large as unigrams, for example) and the time to process the N-grams can balloon dramatically as well.</p>
<p>All that being said, we would suggest focusing on bigrams and trigrams as useful analysis tools.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">textblob</span> <span class="kn">import</span> <span class="n">TextBlob</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">n_grams</span> <span class="o">=</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">ngrams</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">characters</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;he&#39;</span><span class="p">,</span> <span class="s1">&#39;she&#39;</span><span class="p">,</span> <span class="s1">&#39;it&#39;</span><span class="p">,</span> <span class="s1">&#39;they&#39;</span><span class="p">]:</span>
     <span class="n">characters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">n</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">characters</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">ngram_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">ngrams</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ngram_counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="term-frequency">
<h2>Term Frequency<a class="headerlink" href="#term-frequency" title="Permalink to this heading">#</a></h2>
<p>Term Frequency is a measure of how often a term appears in a document. There are different ways to define this but the simplest is a raw count of the number of times each term appears.</p>
<p>There are other ways of defining this including a true term frequency and a log scaled definition. All three have been implemented below but the default will the raw count definition, as it matches with the remainder of the definitions in this tutorial.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Definition</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Raw Count</p></td>
<td><p>$<span class="math notranslate nohighlight">\(f_{t,d}\)</span>$</p></td>
</tr>
<tr class="row-odd"><td><p>Term Frequency</p></td>
<td><p>$<span class="math notranslate nohighlight">\(\frac{f_{t,d}}{\sum_{t'\in d}f_{t',d}}\)</span>$</p></td>
</tr>
<tr class="row-even"><td><p>Log Scaled</p></td>
<td><p>$<span class="math notranslate nohighlight">\(\log(1+f_{t,d})\)</span>$</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## Raw Count Definition</span>
<span class="n">tf1</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">])</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>

<span class="c1">## Term Frequency Definition</span>
<span class="c1">#tf1 = (data[&#39;Story&#39;][0:5]).apply(lambda x: (pd.value_counts(x.split(&quot; &quot;)))/len(x.split(&quot; &quot;))).sum(axis = 0).reset_index() </span>

<span class="c1">## Log Scaled Definition</span>
<span class="c1">#tf1 = (data[&#39;Story&#39;][0:10]).apply(lambda x: 1.0+np.log(pd.value_counts(x.split(&quot; &quot;)))).sum(axis = 0).reset_index() </span>

<span class="n">tf1</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">,</span><span class="s1">&#39;tf&#39;</span><span class="p">]</span>
<span class="n">tf1</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="inverse-document-frequency">
<h2>Inverse Document Frequency<a class="headerlink" href="#inverse-document-frequency" title="Permalink to this heading">#</a></h2>
<p>Inverse Document Frequency is a measure of how common or rare a term is across multiple documents. That gives a measure of how much weight that term carries.</p>
<p>For a more concrete analogy of this, imagine a room full of NBA players; here a 7 foot tall person wouldn’t be all that shocking. However if you have a room full of kindergarten students, a 7 foot tall person would be a huge surprise.</p>
<p>The simplest and standard definition of Inverse Document Frequency is to take the logarithm of the ratio of the number of documents containing a term to the total number of documents.</p>
<div class="math notranslate nohighlight">
\[-\log\frac{n_t}{N} = \log\frac{N}{n_t}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tf1</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">]):</span>
    <span class="n">tf1</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="s1">&#39;idf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">word</span><span class="p">)])))</span>

<span class="n">tf1</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="term-frequency-inverse-document-frequency-tf-idf">
<h2>Term Frequency – Inverse Document Frequency (<a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a>)<a class="headerlink" href="#term-frequency-inverse-document-frequency-tf-idf" title="Permalink to this heading">#</a></h2>
<p>Term Frequency – Inverse Document Frequency (TF-IDF) is a composite measure of both Term Frequency and Inverse Document Frequency.</p>
<p>From <a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">Wikipedia</a>:
“A high weight in TF–IDF is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms”</p>
<p>More concisely, a high TD-IDF says that a word is very important in the documents in which it appears.</p>
<p>There are a few weighting schemes for TF-IDF. Here we use scheme (1).</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Weighting Scheme</p></th>
<th class="head"><p>Document Term Weight</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(1)</p></td>
<td><p>$<span class="math notranslate nohighlight">\(f_{t,d}\cdot\log\frac{N}{n_t}\)</span>$</p></td>
</tr>
<tr class="row-odd"><td><p>(2)</p></td>
<td><p>$<span class="math notranslate nohighlight">\(1+\log(f_{t,d})\)</span>$</p></td>
</tr>
<tr class="row-even"><td><p>(3)</p></td>
<td><p>$<span class="math notranslate nohighlight">\((1+\log(f_{t,d}))\cdot\log\frac{N}{n_t}\)</span>$</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf1</span><span class="p">[</span><span class="s1">&#39;tfidf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf1</span><span class="p">[</span><span class="s1">&#39;tf&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">tf1</span><span class="p">[</span><span class="s1">&#39;idf&#39;</span><span class="p">]</span>
<span class="n">tf1</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;tfidf&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
<p>It is worth noting that the <em>sklearn</em> library has the ability to directly calculate a TD-IDF matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;word&#39;</span><span class="p">,</span>
 <span class="n">stop_words</span><span class="o">=</span> <span class="s1">&#39;english&#39;</span><span class="p">,</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">data_vect</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">])</span>

<span class="n">data_vect</span>
</pre></div>
</div>
</section>
<section id="similarity">
<h2>Similarity<a class="headerlink" href="#similarity" title="Permalink to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span> <span class="c1"># This suppresses a warning from scikit learn that they are going to update their code</span>
</pre></div>
</div>
<p>One thing we can look at is how similar two texts are. This has a practical use when looking for plagiarism, but can also be used to compare author’s styles. To do this there are a few ways we can measure similarity.</p>
<p>First we need to set up our two sets of texts. Here we have the ability to choose the size of our sets and whether we want the first n texts from our full data set or just a random sample. Keep in mind that if you want to use two dissimilar sets, you won’t have a control value (something x itself).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">size</span><span class="o">=</span><span class="mi">10</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">## First n by First n</span>
<span class="c1">#set1 = data[[&quot;Index&quot;,&#39;Story&#39;,&#39;Title&#39;]][:size]</span>
<span class="c1">#set2 = data[[&quot;Index&quot;,&#39;Story&#39;,&#39;Title&#39;]][:size]</span>

<span class="c1">## Random X Itself</span>
<span class="n">set1</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s2">&quot;Index&quot;</span><span class="p">,</span><span class="s1">&#39;Story&#39;</span><span class="p">,</span><span class="s1">&#39;Title&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="n">set2</span> <span class="o">=</span> <span class="n">set1</span>

<span class="c1">## First n by Random</span>
<span class="c1">#set1 = data[[&quot;Index&quot;,&#39;Story&#39;,&#39;Title&#39;]][:size]</span>
<span class="c1">#set2 = data[[&quot;Index&quot;,&#39;Story&#39;,&#39;Title&#39;]].sample(size)</span>

<span class="c1">## Random X Random</span>
<span class="c1">#set1 = data[[&quot;Index&quot;,&#39;Story&#39;,&#39;Title&#39;]].sample(size)</span>
<span class="c1">#set2 = data[[&quot;Index&quot;,&#39;Story&#39;,&#39;Title&#39;]].sample(size)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#To let us do a &quot;cross join&quot;: Every row in one gets matched to all rows in the other</span>
<span class="n">set1</span><span class="p">[</span><span class="s1">&#39;key&#39;</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span> 
<span class="n">set2</span><span class="p">[</span><span class="s1">&#39;key&#39;</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>

<span class="n">similarity</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">set1</span><span class="p">,</span> <span class="n">set2</span><span class="p">,</span> <span class="n">on</span> <span class="o">=</span><span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="n">suffixes</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;_1&#39;</span><span class="p">,</span> <span class="s1">&#39;_2&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<section id="jaccard-similarity">
<h3>Jaccard Similarity<a class="headerlink" href="#jaccard-similarity" title="Permalink to this heading">#</a></h3>
<p>The first is using a metric called the <a class="reference external" href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard Index</a>. This is just taking the intersection of two sets of things (in our case, words or n-grams) and dividing it by the union of those sets. This gives us a metric for understanding how the word usage compares but doesn’t account for repeated words since the union and intersections just take unique words. One advantage though is that we can easily extend the single word similarity to compare bi-grams and other n-grams if we want to examine phrase usage.</p>
<div class="math notranslate nohighlight">
\[S_{J}(A,B)=\frac{A \cap B}{A \cup B}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">jaccard</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    
    <span class="n">old</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Story_1&#39;</span><span class="p">]</span>
    <span class="n">new</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Story_2&#39;</span><span class="p">]</span>
    
    <span class="n">old_n_grams</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">el</span><span class="p">)</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">old</span><span class="p">)</span><span class="o">.</span><span class="n">ngrams</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
    <span class="n">new_n_grams</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">el</span><span class="p">)</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">new</span><span class="p">)</span><span class="o">.</span><span class="n">ngrams</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>
        
    <span class="n">union</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">old_n_grams</span><span class="p">)</span> <span class="o">|</span> <span class="nb">set</span><span class="p">(</span><span class="n">new_n_grams</span><span class="p">))</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">old_n_grams</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">new_n_grams</span><span class="p">))</span>
    
    <span class="n">lu</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">union</span><span class="p">)</span>
    <span class="n">li</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="n">li</span><span class="o">/</span><span class="n">lu</span><span class="p">,</span><span class="n">li</span><span class="p">,</span><span class="n">lu</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]:</span> <span class="c1"># Add values to the list for the n-gram you&#39;re interested in</span>
    <span class="n">similarity</span><span class="p">[</span><span class="s1">&#39;Jaccard_Index_for_</span><span class="si">{}</span><span class="s1">_grams&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span><span class="o">=</span><span class="n">similarity</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jaccard</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="p">[</span><span class="s1">&#39;Intersection&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">similarity</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jaccard</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="p">[</span><span class="s1">&#39;Union&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">similarity</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jaccard</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">2</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="cosine-similarity">
<h3>Cosine Similarity<a class="headerlink" href="#cosine-similarity" title="Permalink to this heading">#</a></h3>
<p>The second metric we can use is <a class="reference external" href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine Similarity</a>, however there is a catch here. Cosine similarity requires a vector for each word so we make a choice here to use term frequency. You could choose something else, inverse document frequency or tf-idf would both be good choices. Cosine similarity with a term frequency vector gives us something very similar to the Jaccard Index but accounts for word repetition. This makes it better for tracking word importance between two texts.</p>
<div class="math notranslate nohighlight">
\[S_{C}(v_1,v_2)=cos(\theta)=\frac{v_1\cdot v_2}{||v_1||\times||v_2||}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_cosine_sim</span><span class="p">(</span><span class="n">str1</span><span class="p">,</span><span class="n">str2</span><span class="p">):</span> 
    <span class="n">vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">get_vectors</span><span class="p">([</span><span class="n">str1</span><span class="p">,</span><span class="n">str2</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">get_vectors</span><span class="p">(</span><span class="n">slist</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">slist</span><span class="p">]</span>
    <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
    <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">cosine</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    
    <span class="n">old</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Story_1&#39;</span><span class="p">]</span>
    <span class="n">new</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Story_2&#39;</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">get_cosine_sim</span><span class="p">(</span><span class="n">old</span><span class="p">,</span><span class="n">new</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>   
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="p">[</span><span class="s1">&#39;Cosine_Similarity&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">similarity</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">cosine</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="visualize-similarity">
<h3>Visualize Similarity<a class="headerlink" href="#visualize-similarity" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;Cosine_Similarity&#39;</span>
<span class="c1">#metric = &#39;Jaccard_Index_for_2_grams&#39;</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">similarity</span><span class="p">[</span><span class="s1">&#39;Index_1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">columns</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">similarity</span><span class="p">[</span><span class="s1">&#39;Index_2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="n">sub</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">[(</span><span class="n">similarity</span><span class="p">[</span><span class="s1">&#39;Index_1&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">sub</span><span class="p">[</span><span class="n">sub</span><span class="p">[</span><span class="s1">&#39;Index_2&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">col</span><span class="p">][</span><span class="n">metric</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
<span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="p">[[</span><span class="s1">&#39;Title_1&#39;</span><span class="p">,</span><span class="s1">&#39;Index_1&#39;</span><span class="p">,</span><span class="s1">&#39;Title_2&#39;</span><span class="p">,</span><span class="s1">&#39;Index_2&#39;</span><span class="p">,</span><span class="n">metric</span><span class="p">]]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span><span class="p">[</span><span class="n">similarity</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span><span class="o">&lt;</span><span class="mf">0.999</span><span class="p">][[</span><span class="s1">&#39;Title_1&#39;</span><span class="p">,</span><span class="s1">&#39;Index_1&#39;</span><span class="p">,</span><span class="s1">&#39;Title_2&#39;</span><span class="p">,</span><span class="s1">&#39;Index_2&#39;</span><span class="p">,</span><span class="n">metric</span><span class="p">]]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">12</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="bag-of-words">
<h2><a class="reference external" href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words</a><a class="headerlink" href="#bag-of-words" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words</a> a way to represent text based on the idea that similar texts will contain similar vocabulary. There is a lot to this model and we provide merely a simple implementation of it here.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">bow</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">analyzer</span> <span class="o">=</span> <span class="s2">&quot;word&quot;</span><span class="p">)</span>
<span class="n">data_bow</span> <span class="o">=</span> <span class="n">bow</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">])</span>
<span class="n">data_bow</span>
</pre></div>
</div>
</section>
<section id="sentiment-analysis">
<h2>Sentiment Analysis<a class="headerlink" href="#sentiment-analysis" title="Permalink to this heading">#</a></h2>
<p>Sentiment is a way of measuring the overall positivity or negativity in a given text.</p>
<p>To do this we will use the built in sentiment function in the <em>TextBlob</em> package. This function will return the polarity and subjectivity scores for each data chunk.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sentiment</span><span class="p">)</span>
</pre></div>
</div>
<p>Focusing on the polarity score, we are able to see the overall sentiment of each data chunk. The closer to 1 the more positive and the closer to -1 the more negative.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">TextBlob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sentiment</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">)</span>
<span class="n">data</span><span class="p">[[</span><span class="s1">&#39;Story&#39;</span><span class="p">,</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>Here we have textted the sentiment scores for the first 10 chunks.</p>
<p>Notice they tend to be positive but not exceedingly so.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_1</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]][:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we have sorted and textted all of the sentiment scores for the chunks in our database.</p>
<p>We can clearly see that most of the text data is positive but not overwhelmingly so (as seen by the long tail of the distribution). However, the parts that are negative tend to be more polarized than the positive ones (a shorter tail and sharper peak).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_2</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;sentiment&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-tf-idf-and-machine-learning">
<h2>Using TF-IDF and Machine Learning<a class="headerlink" href="#using-tf-idf-and-machine-learning" title="Permalink to this heading">#</a></h2>
<p>This is significantly more advanced than the rest of the tutorial. This takes the TF-IDF matrix and applies a <a class="reference external" href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering algorithm</a>. This groups the texts into clusters of similar terms from the TF-IDF matrix. This algorithm randomly seeds X “means”, the values are then clustered into the nearest mean. The centroid of the values in each cluster then becomes the new mean and the process repeats until a convergence is reached.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="n">groups</span> <span class="o">=</span> <span class="mi">15</span>

<span class="n">num_clusters</span> <span class="o">=</span> <span class="n">groups</span>
<span class="n">num_seeds</span> <span class="o">=</span> <span class="n">groups</span>
<span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;nipy_spectral&quot;</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span> <span class="c1"># Builds a discrete color mapping using a built in matplotlib color map</span>

<span class="n">c</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cmap</span><span class="o">.</span><span class="n">N</span><span class="p">):</span> <span class="c1"># Converts our discrete map into Hex Values</span>
    <span class="n">rgba</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">c</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">i</span><span class="p">:</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">rgb2hex</span><span class="p">(</span><span class="n">rgba</span><span class="p">)})</span>

<span class="n">labels_color_map</span><span class="o">=</span><span class="n">c</span>

<span class="n">pca_num_components</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tsne_num_components</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># calculate tf-idf of texts</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;word&#39;</span><span class="p">,</span><span class="n">stop_words</span><span class="o">=</span> <span class="s1">&#39;english&#39;</span><span class="p">,</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">tf_idf_matrix</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story&#39;</span><span class="p">])</span>

<span class="c1"># create k-means model with custom config</span>
<span class="n">clustering_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span>
    <span class="n">n_clusters</span><span class="o">=</span><span class="n">num_clusters</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iterations</span><span class="p">,</span>
    <span class="c1">#precompute_distances=&quot;auto&quot;,</span>
    <span class="c1">#n_jobs=-1</span>
<span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">clustering_model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">tf_idf_matrix</span><span class="p">)</span>
<span class="c1">#print(labels)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tf_idf_matrix</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>

<span class="c1"># ----------------------------------------------------------------------------------------------------------------------</span>

<span class="n">reduced_data</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">pca_num_components</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># print(reduced_data)</span>

<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>
<span class="n">legendlist</span><span class="o">=</span><span class="p">[</span><span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">labels_color_map</span><span class="p">[</span><span class="n">key</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">))</span><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">labels_color_map</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">instance</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reduced_data</span><span class="p">):</span>
    <span class="c1">#print(instance, index, labels[index])</span>
    <span class="n">pca_comp_1</span><span class="p">,</span> <span class="n">pca_comp_2</span> <span class="o">=</span> <span class="n">reduced_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="n">color</span> <span class="o">=</span> <span class="n">labels_color_map</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pca_comp_1</span><span class="p">,</span> <span class="n">pca_comp_2</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">legendlist</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>



<span class="c1"># t-SNE plot</span>
<span class="c1">#embeddings = TSNE(n_components=tsne_num_components)</span>
<span class="c1">#Y = embeddings.fit_transform(X)</span>
<span class="c1">#plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)</span>
<span class="c1">#plt.show()</span>

</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_test</span> <span class="o">=</span> <span class="n">tf1</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;tfidf&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">1000</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story Type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story Type&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">title_groups</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="n">labels</span><span class="p">,</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Title&#39;</span><span class="p">],</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Story Type&#39;</span><span class="p">]])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#title_groups = np.transpose([labels,data[&#39;Title&#39;]])</span>
</pre></div>
</div>
<p>These are the titles of the texts in each cluster. Keep in mind that each time you run the algorithm, the randomness in generating the initial means will result in different clusters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">title_groups</span><span class="p">)):</span>
    <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="s1">&#39;Group&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">title_groups</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grp</span><span class="o">=</span><span class="mi">0</span>
<span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Group&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">grp</span><span class="p">][</span><span class="s1">&#39;Title&#39;</span><span class="p">]</span><span class="c1">#.sample(15)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">groups</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;#### </span><span class="si">{}</span><span class="s2"> ###&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">title_groups</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">el</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">el</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./code"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="12_workbook.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Data Visualization</p>
      </div>
    </a>
    <a class="right-next"
       href="04_assignment.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Data Visualization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Intermediate TextMining with Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-words-and-characters">Counting Words and Characters</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#processing-text">Processing Text</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cleaning-up-words">Cleaning Up Words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lowercase">Lowercase</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-punctuation">Remove Punctuation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-stopwords">Remove Stopwords</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#remove-frequent-words">Remove Frequent Words</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lemmatization">Lemmatization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-text-processing">Advanced Text Processing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams">N-grams</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency">Term Frequency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-document-frequency">Inverse Document Frequency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-inverse-document-frequency-tf-idf">Term Frequency – Inverse Document Frequency (TF-IDF)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity">Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jaccard-similarity">Jaccard Similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine Similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualize-similarity">Visualize Similarity</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">Bag of Words</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis">Sentiment Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-tf-idf-and-machine-learning">Using TF-IDF and Machine Learning</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>